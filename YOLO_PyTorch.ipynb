{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import glob\n",
    "import xmltodict\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_images_path     = '../VOCdevkit/VOC2012/JPEGImages'\n",
    "data_annotation_path = '../VOCdevkit/VOC2012/Annotations'\n",
    "trained_model_path = './trained_model/'\n",
    "image_sizes = [320,352,384,416,448,480,512,544,570,608]\n",
    "image_depth  = 3\n",
    "detection_conv_size = 3\n",
    "subsampled_ratio = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General information about dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the image and annotation file paths\n",
    "list_images      = sorted([x for x in glob.glob(data_images_path + '/**')])     #length : 17125\n",
    "list_annotations = sorted([x for x in glob.glob(data_annotation_path + '/**')]) #length : 17125\n",
    "total_images = len(list_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(xml_files=list_annotations):\n",
    "    '''\n",
    "    Output: All the distinct classes for this dataset.\n",
    "    \n",
    "    '''\n",
    "    classes = []\n",
    "    \n",
    "    for file in xml_files: \n",
    "\n",
    "        f = open(file)\n",
    "        doc = xmltodict.parse(f.read()) #parse the xml file to python dict.\n",
    "\n",
    "        #Images in the dataset might contain either 1 object or more than 1 object. For images with 1 object, the annotation for the object\n",
    "        #in the xml file will be located in 'annotation' -> 'object' -> 'name'. For images with more than 1 object, the annotations for the objects\n",
    "        #will be nested in 'annotation' -> 'object' thus requiring a loop to iterate through them. (Pascal VOC format)\n",
    "\n",
    "        try: \n",
    "            #try iterating through the tag. (For images with more than 1 obj.)\n",
    "            for obj in doc['annotation']['object']:\n",
    "                classes.append(obj['name'].lower()) #append the lowercased string.\n",
    "\n",
    "        except TypeError as e: #iterating through non-nested tags would throw a TypeError.\n",
    "            classes.append(doc['annotation']['object']['name'].lower()) #append the lowercased string.\n",
    "\n",
    "        f.close()\n",
    "\n",
    "    classes = list(set(classes)) #remove duplicates.\n",
    "    classes.sort()\n",
    "\n",
    "    #returns a list containing the names of classes after being sorted.\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = get_classes()\n",
    "num_of_class = len(classes)\n",
    "excluded_classes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_xml(xml_file_path, num_of_class = num_of_class):\n",
    "    '''\n",
    "    Input : A SINGLE xml file and the total number of classes in the dataset. \n",
    "    Output: Labels in numpy array format (Object classes their corresponding bounding box coordinates).\n",
    "\n",
    "    Desc : This function parses a single xml file and outputs the objects classes and their corresponding bounding box coordinates\n",
    "           [top-left-x, top-left-y, btm-right-x, btm-right-y] on the resized image.\n",
    "\n",
    "    '''\n",
    "\n",
    "    f = open(xml_file_path)\n",
    "    doc = xmltodict.parse(f.read()) #parse the xml file to python dict.\n",
    "\n",
    "    #get the original image height and width. Images have different height and width from each other.\n",
    "    ori_img_height = float(doc['annotation']['size']['height'])\n",
    "    ori_img_width  = float(doc['annotation']['size']['width'])\n",
    "\n",
    "\n",
    "    class_label = [] #init for keeping track objects' labels.\n",
    "    bbox_label  = [] #init for keeping track of objects' bounding box (bb).\n",
    "\n",
    "\n",
    "    #Images in the dataset might contain either 1 object or more than 1 object. For images with 1 object, the annotation for the object\n",
    "    #in the xml file will be located in 'annotation' -> 'object' -> 'name'. For images with more than 1 object, the annotations for the objects\n",
    "    #will be nested in 'annotation' -> 'object' thus requiring a loop to iterate through them. (Pascal VOC format)\n",
    "    try:\n",
    "        #Try iterating through the tag (For images with more than 1 obj).\n",
    "        for each_obj in doc['annotation']['object']:\n",
    "            \n",
    "            obj_class = each_obj['name'].lower() #get the label for the object and lowercase the string.\n",
    "            \n",
    "            if obj_class in excluded_classes:\n",
    "                continue\n",
    "\n",
    "            #Pascal VOC's format to denote bounding boxes are to denote the top left part of the box and the bottom right of the box.\n",
    "            #the coordinates are in terms of x and y axis for both part of the box.\n",
    "            x_min = float(each_obj['bndbox']['xmin']) #top left x-axis coordinate.\n",
    "            x_max = float(each_obj['bndbox']['xmax']) #bottom right x-axis coordinate.\n",
    "            y_min = float(each_obj['bndbox']['ymin']) #top left y-axis coordinate.\n",
    "            y_max = float(each_obj['bndbox']['ymax']) #bottom right y-axis coordinate.\n",
    "\n",
    "        ##################################################################################\n",
    "        #We want to make sure the coordinates are resized according to the resized image.#\n",
    "        ##################################################################################\n",
    "\n",
    "            #All the images will be resized to a fixed size in order to be fixed-size inputs to the neural network model.\n",
    "            #Therefore, we need to resize the coordinates as well since the coordinates above is based on the original size of the images.\n",
    "\n",
    "            #In order to find the resized coordinates, we must multiply the ratio of the resized image compared to its original to the coordinates.\n",
    "            x_min = float((image_width/ori_img_width)*x_min)\n",
    "            y_min = float((image_height/ori_img_height)*y_min)\n",
    "            x_max = float((image_width/ori_img_width)*x_max)\n",
    "            y_max = float((image_height/ori_img_height)*y_max)\n",
    "\n",
    "            generated_box_info = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "\n",
    "            index = classes.index(obj_class) #get the index of the object's class.\n",
    "\n",
    "            #append each object's class label and the bounding box label (converted to Faster R-CNN format) into the list initialized earlier.\n",
    "            class_label.append(index)\n",
    "            bbox_label.append(np.asarray(generated_box_info, dtype='float32'))\n",
    "\n",
    "    except TypeError as e : #happens when the iteration through the tag fails due to only 1 object being in the image.\n",
    "\n",
    "        #SAME PROCEDURE AS ABOVE !  \n",
    "\n",
    "        #Getting these information from the XML file differs compared to above,\n",
    "        obj_class = doc['annotation']['object']['name']\n",
    "        \n",
    "        if not obj_class in excluded_classes:\n",
    "                        \n",
    "            x_min = float(doc['annotation']['object']['bndbox']['xmin']) \n",
    "            x_max = float(doc['annotation']['object']['bndbox']['xmax']) \n",
    "            y_min = float(doc['annotation']['object']['bndbox']['ymin']) \n",
    "            y_max = float(doc['annotation']['object']['bndbox']['ymax']) \n",
    "\n",
    "            x_min = float((image_width/ori_img_width)*x_min)\n",
    "            y_min = float((image_height/ori_img_height)*y_min)\n",
    "            x_max = float((image_width/ori_img_width)*x_max)\n",
    "            y_max = float((image_height/ori_img_height)*y_max)\n",
    "\n",
    "            generated_box_info = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "            #Get the index of the class\n",
    "            index = classes.index(obj_class) \n",
    "\n",
    "            class_label.append(index)\n",
    "            bbox_label.append(np.asarray(generated_box_info, dtype='float32'))\n",
    "\n",
    "\n",
    "    return class_label, np.asarray(bbox_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_Means:\n",
    "    \n",
    "    \n",
    "    def __init__(self, k, boxes):\n",
    "        \n",
    "        self.k = k\n",
    "        self.boxes = boxes\n",
    "        self.rows = self.boxes.shape[0]\n",
    "        self.distances = np.empty((self.rows, self.k))\n",
    "        self.last_centroids = np.zeros((self.rows,))\n",
    "        \n",
    "        self.boxes = self.process_boxes(self.boxes)\n",
    "        self.centroids = []\n",
    "        for i in range(self.k):\n",
    "            self.centroids.append(self.boxes[i,:])\n",
    "        \n",
    "        self.centroids = np.asarray(self.centroids, dtype=np.float32)\n",
    "        \n",
    "    def process_boxes(self, boxes):\n",
    "        \n",
    "        new_boxes = boxes.copy()\n",
    "        for row in range(self.rows):\n",
    "            \n",
    "            new_boxes[row][2] = np.abs(new_boxes[row][2] - new_boxes[row][0])\n",
    "            new_boxes[row][3] = np.abs(new_boxes[row][3] - new_boxes[row][1])\n",
    "        \n",
    "        return np.delete(new_boxes, [0,1], axis=1)\n",
    "    \n",
    "    def iou(self, box, centroids):\n",
    "        \n",
    "        x = np.minimum(centroids[:, 0], box[0])\n",
    "        y = np.minimum(centroids[:, 1], box[1])\n",
    "        \n",
    "        if np.count_nonzero(x == 0) > 0 or np.count_nonzero(y == 0) > 0:\n",
    "            raise ValueError(\"The given box has no area!\")\n",
    "        \n",
    "        intersection_area = x * y\n",
    "        box_area = box[0] * box[1]\n",
    "        centroid_area = centroids[:, 0] * centroids[:, 1]\n",
    "        \n",
    "        IoUs = intersection_area / (box_area + centroid_area - intersection_area)\n",
    "        \n",
    "        return IoUs\n",
    "    \n",
    "    def __call__(self):\n",
    "        \n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            for row in range(self.rows):\n",
    "                self.distances[row] = 1 - self.iou(self.boxes[row], self.centroids)\n",
    "            \n",
    "            nearest_centroids = np.argmin(self.distances, axis=1)\n",
    "            \n",
    "            if (self.last_centroids == nearest_centroids).all():\n",
    "                break\n",
    "            \n",
    "            for cluster in range(self.k):\n",
    "                self.centroids[cluster] = np.mean(self.boxes[nearest_centroids == cluster], axis=0)\n",
    "                \n",
    "            self.last_centroids = nearest_centroids\n",
    "        \n",
    "        return self.centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_anchors(k=5, total_images=total_images,):\n",
    "    \n",
    "    gt_boxes_array = []\n",
    "    object_labels_array = []\n",
    "    \n",
    "    for i in range(total_images):\n",
    "        \n",
    "        object_labels, gt_boxes = get_labels_from_xml(xml_file_path=list_annotations[i])\n",
    "        \n",
    "        if len(gt_boxes) == 1:\n",
    "            gt_boxes_array.append(np.asarray(gt_boxes[0], dtype=np.float32))\n",
    "            object_labels_array.append(object_labels[0])\n",
    "        else:\n",
    "            for j in range(len(gt_boxes)):\n",
    "                gt_boxes_array.append(np.asarray(gt_boxes[j], dtype=np.float32))\n",
    "                object_labels_array.append(object_labels[j])\n",
    "                \n",
    "    gt_boxes_array = np.asarray(gt_boxes_array, dtype=np.float32)\n",
    "    \n",
    "    kmeans = K_Means(k=k, boxes=gt_boxes_array)\n",
    "    anchor_sizes = kmeans()\n",
    "    anchor_sizes = np.asarray(anchor_sizes, dtype=np.float32)\n",
    "    \n",
    "    \n",
    "    return anchor_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_width' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d444e1725987>\u001b[0m in \u001b[0;36mget_labels_from_xml\u001b[0;34m(xml_file_path, num_of_class)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mobj_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meach_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#get the label for the object and lowercase the string.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b63cf99a5510>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manchor_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-0c47e9b61b79>\u001b[0m in \u001b[0;36mcluster_anchors\u001b[0;34m(k, total_images)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mobject_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_labels_from_xml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_file_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist_annotations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_boxes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-d444e1725987>\u001b[0m in \u001b[0;36mget_labels_from_xml\u001b[0;34m(xml_file_path, num_of_class)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0my_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bndbox'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ymax'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mx_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_width\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mori_img_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0my_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_height\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mori_img_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mx_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_width\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mori_img_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_width' is not defined"
     ]
    }
   ],
   "source": [
    "anchor_sizes = cluster_anchors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
