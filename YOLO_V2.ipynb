{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as NN\n",
    "from torch.optim import Adam, SGD\n",
    "import time\n",
    "import glob\n",
    "import xmltodict\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_images_path     = '../VOCdevkit/VOC2012/JPEGImages'\n",
    "data_annotation_path = '../VOCdevkit/VOC2012/Annotations'\n",
    "trained_model_path = './trained_model/'\n",
    "image_sizes = [320,352,384,416,448,480,512,544,570,608]\n",
    "image_height = image_sizes[0]\n",
    "image_width = image_sizes[0]\n",
    "image_depth  = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the image and annotation file paths\n",
    "list_images      = sorted([x for x in glob.glob(data_images_path + '/**')])     #length : 17125\n",
    "list_annotations = sorted([x for x in glob.glob(data_annotation_path + '/**')]) #length : 17125\n",
    "total_images = len(list_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(xml_files=list_annotations):\n",
    "    '''\n",
    "    Output: All the distinct classes for this dataset.\n",
    "    \n",
    "    '''\n",
    "    classes = []\n",
    "    \n",
    "    for file in xml_files: \n",
    "\n",
    "        f = open(file)\n",
    "        doc = xmltodict.parse(f.read()) #parse the xml file to python dict.\n",
    "\n",
    "        #Images in the dataset might contain either 1 object or more than 1 object. For images with 1 object, the annotation for the object\n",
    "        #in the xml file will be located in 'annotation' -> 'object' -> 'name'. For images with more than 1 object, the annotations for the objects\n",
    "        #will be nested in 'annotation' -> 'object' thus requiring a loop to iterate through them. (Pascal VOC format)\n",
    "\n",
    "        try: \n",
    "            #try iterating through the tag. (For images with more than 1 obj.)\n",
    "            for obj in doc['annotation']['object']:\n",
    "                classes.append(obj['name'].lower()) #append the lowercased string.\n",
    "\n",
    "        except TypeError as e: #iterating through non-nested tags would throw a TypeError.\n",
    "            classes.append(doc['annotation']['object']['name'].lower()) #append the lowercased string.\n",
    "\n",
    "        f.close()\n",
    "\n",
    "    classes = list(set(classes)) #remove duplicates.\n",
    "    classes.sort()\n",
    "\n",
    "    #returns a list containing the names of classes after being sorted.\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = get_classes()\n",
    "num_of_class = len(classes)\n",
    "excluded_classes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(xml_files=list_annotations):\n",
    "    '''\n",
    "    Output: All the distinct classes for this dataset.\n",
    "    \n",
    "    '''\n",
    "    classes = []\n",
    "    \n",
    "    for file in xml_files: \n",
    "\n",
    "        f = open(file)\n",
    "        doc = xmltodict.parse(f.read()) #parse the xml file to python dict.\n",
    "\n",
    "        #Images in the dataset might contain either 1 object or more than 1 object. For images with 1 object, the annotation for the object\n",
    "        #in the xml file will be located in 'annotation' -> 'object' -> 'name'. For images with more than 1 object, the annotations for the objects\n",
    "        #will be nested in 'annotation' -> 'object' thus requiring a loop to iterate through them. (Pascal VOC format)\n",
    "\n",
    "        try: \n",
    "            #try iterating through the tag. (For images with more than 1 obj.)\n",
    "            for obj in doc['annotation']['object']:\n",
    "                classes.append(obj['name'].lower()) #append the lowercased string.\n",
    "\n",
    "        except TypeError as e: #iterating through non-nested tags would throw a TypeError.\n",
    "            classes.append(doc['annotation']['object']['name'].lower()) #append the lowercased string.\n",
    "\n",
    "        f.close()\n",
    "\n",
    "    classes = list(set(classes)) #remove duplicates.\n",
    "    classes.sort()\n",
    "\n",
    "    #returns a list containing the names of classes after being sorted.\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_xml(xml_file_path, num_of_class = num_of_class):\n",
    "    '''\n",
    "    Input : A SINGLE xml file and the total number of classes in the dataset. \n",
    "    Output: Labels in numpy array format (Object classes their corresponding bounding box coordinates).\n",
    "\n",
    "    Desc : This function parses a single xml file and outputs the objects classes and their corresponding bounding box coordinates\n",
    "           [top-left-x, top-left-y, btm-right-x, btm-right-y] on the resized image.\n",
    "\n",
    "    '''\n",
    "\n",
    "    f = open(xml_file_path)\n",
    "    doc = xmltodict.parse(f.read()) #parse the xml file to python dict.\n",
    "\n",
    "    #get the original image height and width. Images have different height and width from each other.\n",
    "    ori_img_height = float(doc['annotation']['size']['height'])\n",
    "    ori_img_width  = float(doc['annotation']['size']['width'])\n",
    "\n",
    "\n",
    "    class_label = [] #init for keeping track objects' labels.\n",
    "    bbox_label  = [] #init for keeping track of objects' bounding box (bb).\n",
    "\n",
    "\n",
    "    #Images in the dataset might contain either 1 object or more than 1 object. For images with 1 object, the annotation for the object\n",
    "    #in the xml file will be located in 'annotation' -> 'object' -> 'name'. For images with more than 1 object, the annotations for the objects\n",
    "    #will be nested in 'annotation' -> 'object' thus requiring a loop to iterate through them. (Pascal VOC format)\n",
    "    try:\n",
    "        #Try iterating through the tag (For images with more than 1 obj).\n",
    "        for each_obj in doc['annotation']['object']:\n",
    "            \n",
    "            obj_class = each_obj['name'].lower() #get the label for the object and lowercase the string.\n",
    "            \n",
    "            if obj_class in excluded_classes:\n",
    "                continue\n",
    "\n",
    "            #Pascal VOC's format to denote bounding boxes are to denote the top left part of the box and the bottom right of the box.\n",
    "            #the coordinates are in terms of x and y axis for both part of the box.\n",
    "            x_min = float(each_obj['bndbox']['xmin']) #top left x-axis coordinate.\n",
    "            x_max = float(each_obj['bndbox']['xmax']) #bottom right x-axis coordinate.\n",
    "            y_min = float(each_obj['bndbox']['ymin']) #top left y-axis coordinate.\n",
    "            y_max = float(each_obj['bndbox']['ymax']) #bottom right y-axis coordinate.\n",
    "\n",
    "        ##################################################################################\n",
    "        #We want to make sure the coordinates are resized according to the resized image.#\n",
    "        ##################################################################################\n",
    "\n",
    "            #All the images will be resized to a fixed size in order to be fixed-size inputs to the neural network model.\n",
    "            #Therefore, we need to resize the coordinates as well since the coordinates above is based on the original size of the images.\n",
    "\n",
    "            #In order to find the resized coordinates, we must multiply the ratio of the resized image compared to its original to the coordinates.\n",
    "            x_min = float((image_width/ori_img_width)*x_min)\n",
    "            y_min = float((image_height/ori_img_height)*y_min)\n",
    "            x_max = float((image_width/ori_img_width)*x_max)\n",
    "            y_max = float((image_height/ori_img_height)*y_max)\n",
    "            \n",
    "\n",
    "            generated_box_info = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "\n",
    "            index = classes.index(obj_class) #get the index of the object's class.\n",
    "\n",
    "            #append each object's class label and the bounding box label (converted to Faster R-CNN format) into the list initialized earlier.\n",
    "            class_label.append(index)\n",
    "            bbox_label.append(np.asarray(generated_box_info, dtype='float32'))\n",
    "\n",
    "    except TypeError as e : #happens when the iteration through the tag fails due to only 1 object being in the image.\n",
    "\n",
    "        #SAME PROCEDURE AS ABOVE !  \n",
    "\n",
    "        #Getting these information from the XML file differs compared to above,\n",
    "        obj_class = doc['annotation']['object']['name']\n",
    "        \n",
    "        if not obj_class in excluded_classes:\n",
    "                        \n",
    "            x_min = float(doc['annotation']['object']['bndbox']['xmin']) \n",
    "            x_max = float(doc['annotation']['object']['bndbox']['xmax']) \n",
    "            y_min = float(doc['annotation']['object']['bndbox']['ymin']) \n",
    "            y_max = float(doc['annotation']['object']['bndbox']['ymax']) \n",
    "\n",
    "            x_min = float((image_width/ori_img_width)*x_min)\n",
    "            y_min = float((image_height/ori_img_height)*y_min)\n",
    "            x_max = float((image_width/ori_img_width)*x_max)\n",
    "            y_max = float((image_height/ori_img_height)*y_max)\n",
    "\n",
    "            generated_box_info = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "            #Get the index of the class\n",
    "            index = classes.index(obj_class) \n",
    "\n",
    "            class_label.append(index)\n",
    "            bbox_label.append(np.asarray(generated_box_info, dtype='float32'))\n",
    "\n",
    "\n",
    "    return class_label, np.asarray(bbox_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_Means:\n",
    "    \n",
    "    \n",
    "    def __init__(self, k, boxes):\n",
    "        \n",
    "        self.k = k\n",
    "        self.boxes = boxes\n",
    "        self.rows = self.boxes.shape[0]\n",
    "        self.distances = np.empty((self.rows, self.k))\n",
    "        self.last_centroids = np.zeros((self.rows,))\n",
    "        \n",
    "        self.boxes = self.process_boxes(self.boxes)\n",
    "        self.centroids = []\n",
    "        for i in range(self.k):\n",
    "            self.centroids.append(self.boxes[i,:])\n",
    "        \n",
    "        self.centroids = np.asarray(self.centroids, dtype=np.float32)\n",
    "        \n",
    "    def process_boxes(self, boxes):\n",
    "        \n",
    "        new_boxes = boxes.copy()\n",
    "        for row in range(self.rows):\n",
    "#             width = np.abs(new_boxes[row][2] - new_boxes[row][0])\n",
    "#             height = np.abs(new_boxes[row][3] - new_boxes[row][1])\n",
    "            \n",
    "#             new_boxes[row][0] = new_boxes[row][0] / width\n",
    "#             new_boxes[row][1] = new_boxes[row][1] / height\n",
    "#             new_boxes[row][2] = new_boxes[row][2] / width\n",
    "#             new_boxes[row][3] = new_boxes[row][3] / height\n",
    "            \n",
    "            new_boxes[row][2] = np.abs(new_boxes[row][2] - new_boxes[row][0])\n",
    "            new_boxes[row][3] = np.abs(new_boxes[row][3] - new_boxes[row][1])\n",
    "        \n",
    "        return np.delete(new_boxes, [0,1], axis=1)\n",
    "    \n",
    "    def iou(self, box, centroids):\n",
    "        \n",
    "        x = np.minimum(centroids[:, 0], box[0])\n",
    "        y = np.minimum(centroids[:, 1], box[1])\n",
    "        \n",
    "        if np.count_nonzero(x == 0) > 0 or np.count_nonzero(y == 0) > 0:\n",
    "            raise ValueError(\"The given box has no area!\")\n",
    "        \n",
    "        intersection_area = x * y\n",
    "        box_area = box[0] * box[1]\n",
    "        centroid_area = centroids[:, 0] * centroids[:, 1]\n",
    "        \n",
    "        IoUs = intersection_area / (box_area + centroid_area - intersection_area)\n",
    "        \n",
    "        return IoUs\n",
    "    \n",
    "    def __call__(self):\n",
    "        \n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            for row in range(self.rows):\n",
    "                self.distances[row] = 1 - self.iou(self.boxes[row], self.centroids)\n",
    "            \n",
    "            nearest_centroids = np.argmin(self.distances, axis=1)\n",
    "            \n",
    "            if (self.last_centroids == nearest_centroids).all():\n",
    "                break\n",
    "            \n",
    "            for cluster in range(self.k):\n",
    "                self.centroids[cluster] = np.mean(self.boxes[nearest_centroids == cluster], axis=0)\n",
    "                \n",
    "            self.last_centroids = nearest_centroids\n",
    "        \n",
    "        return self.centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40138"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_boxes_array = []\n",
    "for i in range(total_images):\n",
    "    \n",
    "    true_labels, gt_boxes = get_labels_from_xml(xml_file_path=list_annotations[i])\n",
    "    if len(gt_boxes) == 1:\n",
    "        \n",
    "        gt_boxes_array.append(np.asarray(gt_boxes[0], dtype=np.float32))\n",
    "    else:\n",
    "        for i in range(len(gt_boxes)):\n",
    "            gt_boxes_array.append(np.asarray(gt_boxes[i], dtype=np.float32))\n",
    "\n",
    "len(gt_boxes_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_boxes_array = np.asarray(gt_boxes_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = K_Means(k=5, boxes=gt_boxes_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = kmeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = np.zeros((320,320,3))\n",
    "cv2.rectangle(img, (4,4), (int(n[0][0])+4,int(n[0][1])+4), (255,255,255), 2)\n",
    "cv2.rectangle(img, (4,4), (int(n[1][0])+4,int(n[1][1])+4),  (255,255,0), 2)\n",
    "cv2.rectangle(img, (4,4), (int(n[2][0])+4,int(n[2][1])+4),  (0,0,255), 2)\n",
    "cv2.rectangle(img, (4,4), (int(n[3][0])+4,int(n[3][1])+4),  (255,0,0), 2)\n",
    "cv2.rectangle(img, (4,4), (int(n[4][0])+4,int(n[4][1])+4),  (100,0,50), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(cv2.flip(img.astype(np.uint8),0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38 75]\n",
      " [18 24]\n",
      " [49 92]\n",
      " [29 74]\n",
      " [76 36]\n",
      " [59 27]\n",
      " [67 28]\n",
      " [13 24]\n",
      " [ 3 27]\n",
      " [47 80]] [24.5]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randint(100,size=(10,2))\n",
    "b = np.asarray([24.5])\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.5, 24. , 24.5, 24.5, 24.5, 24.5, 24.5, 24. , 24.5, 24.5])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.minimum(a[:,1], b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
