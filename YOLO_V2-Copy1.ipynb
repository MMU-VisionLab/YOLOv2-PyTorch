{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as NN\n",
    "from torch.optim import Adam, SGD\n",
    "import time\n",
    "import glob\n",
    "import xmltodict\n",
    "import cv2\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_images_path     = '../VOCdevkit/VOC2012/JPEGImages'\n",
    "data_annotation_path = '../VOCdevkit/VOC2012/Annotations'\n",
    "trained_model_path = './trained_model/'\n",
    "image_sizes = [320,352,384,416,448,480,512,544,570,608]\n",
    "image_height = image_sizes[0]\n",
    "image_width = image_sizes[0]\n",
    "image_depth  = 3\n",
    "detection_conv_size = 3\n",
    "subsampled_ratio = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the image and annotation file paths\n",
    "list_images      = sorted([x for x in glob.glob(data_images_path + '/**')])     #length : 17125\n",
    "list_annotations = sorted([x for x in glob.glob(data_annotation_path + '/**')]) #length : 17125\n",
    "total_images = len(list_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(xml_files=list_annotations):\n",
    "    '''\n",
    "    Output: All the distinct classes for this dataset.\n",
    "    \n",
    "    '''\n",
    "    classes = []\n",
    "    \n",
    "    for file in xml_files: \n",
    "\n",
    "        f = open(file)\n",
    "        doc = xmltodict.parse(f.read()) #parse the xml file to python dict.\n",
    "\n",
    "        #Images in the dataset might contain either 1 object or more than 1 object. For images with 1 object, the annotation for the object\n",
    "        #in the xml file will be located in 'annotation' -> 'object' -> 'name'. For images with more than 1 object, the annotations for the objects\n",
    "        #will be nested in 'annotation' -> 'object' thus requiring a loop to iterate through them. (Pascal VOC format)\n",
    "\n",
    "        try: \n",
    "            #try iterating through the tag. (For images with more than 1 obj.)\n",
    "            for obj in doc['annotation']['object']:\n",
    "                classes.append(obj['name'].lower()) #append the lowercased string.\n",
    "\n",
    "        except TypeError as e: #iterating through non-nested tags would throw a TypeError.\n",
    "            classes.append(doc['annotation']['object']['name'].lower()) #append the lowercased string.\n",
    "\n",
    "        f.close()\n",
    "\n",
    "    classes = list(set(classes)) #remove duplicates.\n",
    "    classes.sort()\n",
    "\n",
    "    #returns a list containing the names of classes after being sorted.\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = get_classes()\n",
    "num_of_class = len(classes)\n",
    "excluded_classes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_xml(xml_file_path, num_of_class = num_of_class):\n",
    "    '''\n",
    "    Input : A SINGLE xml file and the total number of classes in the dataset. \n",
    "    Output: Labels in numpy array format (Object classes their corresponding bounding box coordinates).\n",
    "\n",
    "    Desc : This function parses a single xml file and outputs the objects classes and their corresponding bounding box coordinates\n",
    "           [top-left-x, top-left-y, btm-right-x, btm-right-y] on the resized image.\n",
    "\n",
    "    '''\n",
    "\n",
    "    f = open(xml_file_path)\n",
    "    doc = xmltodict.parse(f.read()) #parse the xml file to python dict.\n",
    "\n",
    "    #get the original image height and width. Images have different height and width from each other.\n",
    "    ori_img_height = float(doc['annotation']['size']['height'])\n",
    "    ori_img_width  = float(doc['annotation']['size']['width'])\n",
    "\n",
    "\n",
    "    class_label = [] #init for keeping track objects' labels.\n",
    "    bbox_label  = [] #init for keeping track of objects' bounding box (bb).\n",
    "\n",
    "\n",
    "    #Images in the dataset might contain either 1 object or more than 1 object. For images with 1 object, the annotation for the object\n",
    "    #in the xml file will be located in 'annotation' -> 'object' -> 'name'. For images with more than 1 object, the annotations for the objects\n",
    "    #will be nested in 'annotation' -> 'object' thus requiring a loop to iterate through them. (Pascal VOC format)\n",
    "    try:\n",
    "        #Try iterating through the tag (For images with more than 1 obj).\n",
    "        for each_obj in doc['annotation']['object']:\n",
    "            \n",
    "            obj_class = each_obj['name'].lower() #get the label for the object and lowercase the string.\n",
    "            \n",
    "            if obj_class in excluded_classes:\n",
    "                continue\n",
    "\n",
    "            #Pascal VOC's format to denote bounding boxes are to denote the top left part of the box and the bottom right of the box.\n",
    "            #the coordinates are in terms of x and y axis for both part of the box.\n",
    "            x_min = float(each_obj['bndbox']['xmin']) #top left x-axis coordinate.\n",
    "            x_max = float(each_obj['bndbox']['xmax']) #bottom right x-axis coordinate.\n",
    "            y_min = float(each_obj['bndbox']['ymin']) #top left y-axis coordinate.\n",
    "            y_max = float(each_obj['bndbox']['ymax']) #bottom right y-axis coordinate.\n",
    "\n",
    "        ##################################################################################\n",
    "        #We want to make sure the coordinates are resized according to the resized image.#\n",
    "        ##################################################################################\n",
    "\n",
    "            #All the images will be resized to a fixed size in order to be fixed-size inputs to the neural network model.\n",
    "            #Therefore, we need to resize the coordinates as well since the coordinates above is based on the original size of the images.\n",
    "\n",
    "            #In order to find the resized coordinates, we must multiply the ratio of the resized image compared to its original to the coordinates.\n",
    "            x_min = float((image_width/ori_img_width)*x_min)\n",
    "            y_min = float((image_height/ori_img_height)*y_min)\n",
    "            x_max = float((image_width/ori_img_width)*x_max)\n",
    "            y_max = float((image_height/ori_img_height)*y_max)\n",
    "\n",
    "            generated_box_info = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "\n",
    "            index = classes.index(obj_class) #get the index of the object's class.\n",
    "\n",
    "            #append each object's class label and the bounding box label (converted to Faster R-CNN format) into the list initialized earlier.\n",
    "            class_label.append(index)\n",
    "            bbox_label.append(np.asarray(generated_box_info, dtype='float32'))\n",
    "\n",
    "    except TypeError as e : #happens when the iteration through the tag fails due to only 1 object being in the image.\n",
    "\n",
    "        #SAME PROCEDURE AS ABOVE !  \n",
    "\n",
    "        #Getting these information from the XML file differs compared to above,\n",
    "        obj_class = doc['annotation']['object']['name']\n",
    "        \n",
    "        if not obj_class in excluded_classes:\n",
    "                        \n",
    "            x_min = float(doc['annotation']['object']['bndbox']['xmin']) \n",
    "            x_max = float(doc['annotation']['object']['bndbox']['xmax']) \n",
    "            y_min = float(doc['annotation']['object']['bndbox']['ymin']) \n",
    "            y_max = float(doc['annotation']['object']['bndbox']['ymax']) \n",
    "\n",
    "            x_min = float((image_width/ori_img_width)*x_min)\n",
    "            y_min = float((image_height/ori_img_height)*y_min)\n",
    "            x_max = float((image_width/ori_img_width)*x_max)\n",
    "            y_max = float((image_height/ori_img_height)*y_max)\n",
    "\n",
    "            generated_box_info = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "            #Get the index of the class\n",
    "            index = classes.index(obj_class) \n",
    "\n",
    "            class_label.append(index)\n",
    "            bbox_label.append(np.asarray(generated_box_info, dtype='float32'))\n",
    "\n",
    "\n",
    "    return class_label, np.asarray(bbox_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_Means:\n",
    "    \n",
    "    \n",
    "    def __init__(self, k, boxes):\n",
    "        \n",
    "        self.k = k\n",
    "        self.boxes = boxes\n",
    "        self.rows = self.boxes.shape[0]\n",
    "        self.distances = np.empty((self.rows, self.k))\n",
    "        self.last_centroids = np.zeros((self.rows,))\n",
    "        \n",
    "        self.boxes = self.process_boxes(self.boxes)\n",
    "        self.centroids = []\n",
    "        for i in range(self.k):\n",
    "            self.centroids.append(self.boxes[i,:])\n",
    "        \n",
    "        self.centroids = np.asarray(self.centroids, dtype=np.float32)\n",
    "        \n",
    "    def process_boxes(self, boxes):\n",
    "        \n",
    "        new_boxes = boxes.copy()\n",
    "        for row in range(self.rows):\n",
    "            \n",
    "            new_boxes[row][2] = np.abs(new_boxes[row][2] - new_boxes[row][0])\n",
    "            new_boxes[row][3] = np.abs(new_boxes[row][3] - new_boxes[row][1])\n",
    "        \n",
    "        return np.delete(new_boxes, [0,1], axis=1)\n",
    "    \n",
    "    def iou(self, box, centroids):\n",
    "        \n",
    "        x = np.minimum(centroids[:, 0], box[0])\n",
    "        y = np.minimum(centroids[:, 1], box[1])\n",
    "        \n",
    "        if np.count_nonzero(x == 0) > 0 or np.count_nonzero(y == 0) > 0:\n",
    "            raise ValueError(\"The given box has no area!\")\n",
    "        \n",
    "        intersection_area = x * y\n",
    "        box_area = box[0] * box[1]\n",
    "        centroid_area = centroids[:, 0] * centroids[:, 1]\n",
    "        \n",
    "        IoUs = intersection_area / (box_area + centroid_area - intersection_area)\n",
    "        \n",
    "        return IoUs\n",
    "    \n",
    "    def __call__(self):\n",
    "        \n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            for row in range(self.rows):\n",
    "                self.distances[row] = 1 - self.iou(self.boxes[row], self.centroids)\n",
    "            \n",
    "            nearest_centroids = np.argmin(self.distances, axis=1)\n",
    "            \n",
    "            if (self.last_centroids == nearest_centroids).all():\n",
    "                break\n",
    "            \n",
    "            for cluster in range(self.k):\n",
    "                self.centroids[cluster] = np.mean(self.boxes[nearest_centroids == cluster], axis=0)\n",
    "                \n",
    "            self.last_centroids = nearest_centroids\n",
    "        \n",
    "        return self.centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU_calculator(box1, box2, area1=None, area2=None):\n",
    "    '''\n",
    "    Inputs two boxes (each with top-left-x, top-left-y, btm-right-x, btm-right-y coordinates).\n",
    "    If the area of one or two of the boxes are known, it can be passed here as well.\n",
    "    Calculates the IoU between the two boxes and returns the IoU.\n",
    "    '''\n",
    "    \n",
    "    #Get the area of the boxes\n",
    "    if area1 is None:\n",
    "        #Area of the box1. +1 since index starts from 0.\n",
    "        area1 = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
    "    \n",
    "    if area2 is None:\n",
    "        #Area of the box 2. +1 since index starts from 0.\n",
    "        area2 = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
    "    \n",
    "    #Determine the intersection rectangle.\n",
    "    int_rect_top_left_x = max(box1[0], box2[0])\n",
    "    int_rect_top_left_y = max(box1[1], box2[1])\n",
    "    int_rect_btm_rght_x = min(box1[2], box2[2])\n",
    "    int_rect_btm_rght_y = min(box1[3], box2[3])\n",
    "    \n",
    "    #if the boxes do not intersect, the difference will be < 0. Hence we pick 0 in those cases.\n",
    "    int_rect_area = max(0, int_rect_btm_rght_x - int_rect_top_left_x + 1)*max(0, int_rect_btm_rght_y - int_rect_top_left_y)\n",
    "    \n",
    "    #Calculate the IoU.\n",
    "    try:\n",
    "        intersect_over_union = float(int_rect_area / (area1 + area2 - int_rect_area))\n",
    "    except ZeroDivisionError:\n",
    "        \n",
    "        intersect_over_union = 0\n",
    "        \n",
    "    \n",
    "    return intersect_over_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anchors(anchor_sizes, detection_conv_size=detection_conv_size, subsampled_ratio=subsampled_ratio,\n",
    "                     image_height=image_height, image_width=image_width):\n",
    "    '''\n",
    "    anchors : contains N number of anchors where each element in anchors is a list of width and height of the\n",
    "              anchor.\n",
    "    '''\n",
    "    \n",
    "    subsampled_height = int(image_height/subsampled_ratio)\n",
    "    subsampled_width = int(image_width/subsampled_ratio)\n",
    "    \n",
    "    anchors_list  = np.zeros((subsampled_height, subsampled_width, len(anchor_sizes), 5), dtype=np.float32)\n",
    "    anchor_center = [0, 0]\n",
    "    \n",
    "    #iteration stops when the index goes 1 step beyond the size of the feature map\n",
    "    while (anchor_center != [0, subsampled_height]):\n",
    "        \n",
    "        for index, anchor in enumerate(anchor_sizes):\n",
    "            \n",
    "            anchor_coor = [anchor_center[0]*subsampled_ratio, anchor_center[1]*subsampled_ratio,\n",
    "                           anchor[0], anchor[1]]\n",
    "            \n",
    "            anchors_list[anchor_center[0], anchor_center[1], index, :] = [0] + anchor_coor\n",
    "        \n",
    "        anchor_center[0] += 1\n",
    "        \n",
    "        if anchor_center[0] == subsampled_width :\n",
    "            \n",
    "            anchor_center[1] += 1\n",
    "            anchor_center[0] = 0\n",
    "        \n",
    "        \n",
    "    return anchors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highest_iou_anchor(anchors, gt_box):\n",
    "    \n",
    "    x = np.minimum(anchors[:, -2], gt_box[0])\n",
    "    y = np.minimum(anchors[:, -1], gt_box[1])\n",
    "\n",
    "    if np.count_nonzero(x == 0) > 0 or np.count_nonzero(y == 0) > 0:\n",
    "        raise ValueError(\"The given box has no area!\")\n",
    "\n",
    "    intersection_area = x * y\n",
    "    box_area = gt_box[0] * gt_box[1]\n",
    "    centroid_area = anchors[:, -2] * anchors[:, -1]\n",
    "\n",
    "    IoUs = intersection_area / (box_area + centroid_area - intersection_area)\n",
    "    \n",
    "    highest_iou = np.argmax(IoUs, axis=0)\n",
    "\n",
    "    #choose the next highest index if the chosen index is already assigned with an object\n",
    "    index = 1\n",
    "    while (int(anchors[highest_iou, 0]) != 0):\n",
    "        highest_iou = np.argmax(IoUs[index:], axis=0)\n",
    "        index += 1\n",
    "        if index == len(anchors):\n",
    "            break\n",
    "\n",
    "\n",
    "    return highest_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label(ground_truth_boxes, anchors, class_labels, subsampled_ratio=subsampled_ratio,\n",
    "                    image_height=image_height, image_width=image_width):\n",
    "    \n",
    "    \n",
    "    subsampled_height = int(image_height/subsampled_ratio)\n",
    "    subsampled_width  = int(image_width/subsampled_ratio)\n",
    "    \n",
    "    \n",
    "    num_of_classes = len(class_labels)\n",
    "    \n",
    "    anchor_label_array = np.zeros((subsampled_height, subsampled_width, anchors.shape[-2], 5), dtype=np.float32)\n",
    "    class_label_array  = np.zeros((subsampled_height, subsampled_width, anchors.shape[-2], num_of_classes), dtype=np.float32)\n",
    "    \n",
    "    \n",
    "    for i in range(ground_truth_boxes.shape[0]):\n",
    "        \n",
    "        class_label = class_labels[i]\n",
    "        print(class_label)\n",
    "        print(classes[class_label[0]])\n",
    "        \n",
    "        #Get the ground truth box's coordinates.\n",
    "        gt_box_top_left_x = ground_truth_boxes[i][0]\n",
    "        gt_box_top_left_y = ground_truth_boxes[i][1]\n",
    "        gt_box_btm_rght_x = ground_truth_boxes[i][2]\n",
    "        gt_box_btm_rght_y = ground_truth_boxes[i][3]\n",
    "        \n",
    "        #Calculate the area of the original bounding box.1 is added since the index starts from 0 not 1.\n",
    "        gt_box_height = (gt_box_btm_rght_y - gt_box_top_left_y + 1)\n",
    "        gt_box_width = (gt_box_btm_rght_x - gt_box_top_left_x + 1)\n",
    "        \n",
    "        gt_center_x = (gt_box_top_left_x + (gt_box_btm_rght_x - gt_box_top_left_x)/2)\n",
    "        gt_center_y = (gt_box_top_left_y + (gt_box_btm_rght_y - gt_box_top_left_y)/2)\n",
    "        \n",
    "        transformed_gt_box = [gt_center_x, gt_center_y, gt_box_width, gt_box_height]\n",
    "        #the middle point of this object belongs to \"responsible grid\"\n",
    "        responsible_grid = [int(gt_center_x/subsampled_ratio), int(gt_center_y/subsampled_ratio)]\n",
    "        \n",
    "        prospect_anchors = anchors[responsible_grid[0]][responsible_grid[1]]\n",
    "        \n",
    "        #gets the index of the chosen anchor\n",
    "        gt_box = np.asarray(transformed_gt_box[2:], dtype=np.float32)\n",
    "        chosen_anchor_index = get_highest_iou_anchor(prospect_anchors, gt_box)\n",
    "        \n",
    "        #calculate the regression for the chosen anchor with the gt box\n",
    "        chosen_anchor = prospect_anchors[chosen_anchor_index]\n",
    "        \n",
    "        normalized_anchor = np.asarray([(chosen_anchor[1] - (responsible_grid[0])*subsampled_ratio)/subsampled_ratio,\n",
    "                           (chosen_anchor[2] - (responsible_grid[1])*subsampled_ratio)/subsampled_ratio,\n",
    "                            chosen_anchor[3]/image_width, chosen_anchor[4]/image_height], dtype=np.float32)\n",
    "        \n",
    "        \n",
    "        normalized_gt = np.asarray([(gt_center_x - (responsible_grid[0])*subsampled_ratio)/subsampled_ratio,\n",
    "                         (gt_center_y - (responsible_grid[1])*subsampled_ratio)/subsampled_ratio,\n",
    "                          gt_box_width/image_width, gt_box_height/image_height], dtype=np.float32)\n",
    "        \n",
    "        sigmoid_tx = normalized_gt[0]\n",
    "        sigmoid_ty = normalized_gt[1]\n",
    "        tw = math.log(normalized_gt[2]/normalized_anchor[2])\n",
    "        th = math.log(normalized_gt[3]/normalized_anchor[3])\n",
    "        \n",
    "        regression_values = np.asarray([sigmoid_tx, sigmoid_ty, tw, th], dtype=np.float32)\n",
    "        \n",
    "        anchor_label_array[responsible_grid[0]][responsible_grid[1]][chosen_anchor_index][0] = 1.0\n",
    "        anchor_label_array[responsible_grid[0]][responsible_grid[1]][chosen_anchor_index][1:] = regression_values\n",
    "        \n",
    "        \n",
    "        \n",
    "    return chosen_anchor, normalized_anchor,responsible_grid, normalized_gt, anchor_label_array, regression_values\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_boxes_array = []\n",
    "object_labels_array = []\n",
    "for i in range(total_images):\n",
    "    \n",
    "    true_labels, gt_boxes = get_labels_from_xml(xml_file_path=list_annotations[i])\n",
    "    if len(gt_boxes) == 1:\n",
    "        gt_boxes_array.append(np.asarray(gt_boxes[0], dtype=np.float32))\n",
    "        object_labels_array.append([true_labels[0]])\n",
    "    else:\n",
    "        for i in range(len(gt_boxes)):\n",
    "            gt_boxes_array.append(np.asarray(gt_boxes[i], dtype=np.float32))\n",
    "            object_labels_array.append([true_labels[i]])\n",
    "\n",
    "gt_boxes_array = np.asarray(gt_boxes_array)\n",
    "object_labels_array = np.asarray(object_labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = K_Means(k=5, boxes=gt_boxes_array)\n",
    "anchor_sizes = kmeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_sizes = np.asarray(anchor_sizes, dtype=np.int32)\n",
    "len(anchor_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors_list = generate_anchors(anchor_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_labels_array[test_index:test_index+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14]\n",
      "person\n",
      "[4, 4]\n",
      "[0.6        0.86338806 0.28312498 0.58235997]\n",
      "[0.     0.     0.3375 0.6875]\n",
      "[ 0.6         0.86338806 -0.17567709 -0.1659731 ]\n",
      "(array([4]), array([4]), array([0]))\n",
      "[[ 1.          0.6         0.86338806 -0.17567709 -0.1659731 ]]\n"
     ]
    }
   ],
   "source": [
    "ca,an,rg,gt,m, h = generate_label(gt_boxes_array[test_index:test_index+1],class_labels=object_labels_array, anchors=anchors_list)\n",
    "print(rg)\n",
    "print(gt)\n",
    "print(an)\n",
    "print(h)\n",
    "nonzeroes = np.nonzero(m[:,:,:,0])\n",
    "print(nonzeroes)\n",
    "print(m[nonzeroes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asd\n"
     ]
    }
   ],
   "source": [
    "img = np.zeros((320,320,3))\n",
    "cv2.rectangle(img,(int(ca[1] - ca[3]/2), int(ca[2]-ca[-1]/2)), (int(ca[1] + ca[3]/2), int(ca[2]+ca[-1]/2)), (255,255,255), 2)\n",
    "cv2.rectangle(img,(int(gt_boxes_array[test_index][0]),int(gt_boxes_array[test_index][1])), (int(gt_boxes_array[test_index][2]),int(gt_boxes_array[test_index][3])), (255, 0,0),2)\n",
    "print('asd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(cv2.flip(img.astype(np.uint8),0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
