{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as NN\n",
    "from torch.optim import Adam, SGD\n",
    "import time\n",
    "import glob\n",
    "import xmltodict\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_images_path     = '../VOCdevkit/VOC2012/JPEGImages'\n",
    "data_annotation_path = '../VOCdevkit/VOC2012/Annotations'\n",
    "trained_model_path = './trained_model/'\n",
    "image_sizes = [320,352,384,416,448,480,512,544,570,608]\n",
    "image_height = image_sizes[0]\n",
    "image_width = image_sizes[0]\n",
    "image_depth  = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the image and annotation file paths\n",
    "list_images      = sorted([x for x in glob.glob(data_images_path + '/**')])     #length : 17125\n",
    "list_annotations = sorted([x for x in glob.glob(data_annotation_path + '/**')]) #length : 17125\n",
    "total_images = len(list_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(xml_files=list_annotations):\n",
    "    '''\n",
    "    Output: All the distinct classes for this dataset.\n",
    "    \n",
    "    '''\n",
    "    classes = []\n",
    "    \n",
    "    for file in xml_files: \n",
    "\n",
    "        f = open(file)\n",
    "        doc = xmltodict.parse(f.read()) #parse the xml file to python dict.\n",
    "\n",
    "        #Images in the dataset might contain either 1 object or more than 1 object. For images with 1 object, the annotation for the object\n",
    "        #in the xml file will be located in 'annotation' -> 'object' -> 'name'. For images with more than 1 object, the annotations for the objects\n",
    "        #will be nested in 'annotation' -> 'object' thus requiring a loop to iterate through them. (Pascal VOC format)\n",
    "\n",
    "        try: \n",
    "            #try iterating through the tag. (For images with more than 1 obj.)\n",
    "            for obj in doc['annotation']['object']:\n",
    "                classes.append(obj['name'].lower()) #append the lowercased string.\n",
    "\n",
    "        except TypeError as e: #iterating through non-nested tags would throw a TypeError.\n",
    "            classes.append(doc['annotation']['object']['name'].lower()) #append the lowercased string.\n",
    "\n",
    "        f.close()\n",
    "\n",
    "    classes = list(set(classes)) #remove duplicates.\n",
    "    classes.sort()\n",
    "\n",
    "    #returns a list containing the names of classes after being sorted.\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = get_classes()\n",
    "num_of_class = len(classes)\n",
    "excluded_classes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(xml_files=list_annotations):\n",
    "    '''\n",
    "    Output: All the distinct classes for this dataset.\n",
    "    \n",
    "    '''\n",
    "    classes = []\n",
    "    \n",
    "    for file in xml_files: \n",
    "\n",
    "        f = open(file)\n",
    "        doc = xmltodict.parse(f.read()) #parse the xml file to python dict.\n",
    "\n",
    "        #Images in the dataset might contain either 1 object or more than 1 object. For images with 1 object, the annotation for the object\n",
    "        #in the xml file will be located in 'annotation' -> 'object' -> 'name'. For images with more than 1 object, the annotations for the objects\n",
    "        #will be nested in 'annotation' -> 'object' thus requiring a loop to iterate through them. (Pascal VOC format)\n",
    "\n",
    "        try: \n",
    "            #try iterating through the tag. (For images with more than 1 obj.)\n",
    "            for obj in doc['annotation']['object']:\n",
    "                classes.append(obj['name'].lower()) #append the lowercased string.\n",
    "\n",
    "        except TypeError as e: #iterating through non-nested tags would throw a TypeError.\n",
    "            classes.append(doc['annotation']['object']['name'].lower()) #append the lowercased string.\n",
    "\n",
    "        f.close()\n",
    "\n",
    "    classes = list(set(classes)) #remove duplicates.\n",
    "    classes.sort()\n",
    "\n",
    "    #returns a list containing the names of classes after being sorted.\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_xml(xml_file_path, num_of_class = num_of_class):\n",
    "    '''\n",
    "    Input : A SINGLE xml file and the total number of classes in the dataset. \n",
    "    Output: Labels in numpy array format (Object classes their corresponding bounding box coordinates).\n",
    "\n",
    "    Desc : This function parses a single xml file and outputs the objects classes and their corresponding bounding box coordinates\n",
    "           [top-left-x, top-left-y, btm-right-x, btm-right-y] on the resized image.\n",
    "\n",
    "    '''\n",
    "\n",
    "    f = open(xml_file_path)\n",
    "    doc = xmltodict.parse(f.read()) #parse the xml file to python dict.\n",
    "\n",
    "    #get the original image height and width. Images have different height and width from each other.\n",
    "    ori_img_height = float(doc['annotation']['size']['height'])\n",
    "    ori_img_width  = float(doc['annotation']['size']['width'])\n",
    "\n",
    "\n",
    "    class_label = [] #init for keeping track objects' labels.\n",
    "    bbox_label  = [] #init for keeping track of objects' bounding box (bb).\n",
    "\n",
    "\n",
    "    #Images in the dataset might contain either 1 object or more than 1 object. For images with 1 object, the annotation for the object\n",
    "    #in the xml file will be located in 'annotation' -> 'object' -> 'name'. For images with more than 1 object, the annotations for the objects\n",
    "    #will be nested in 'annotation' -> 'object' thus requiring a loop to iterate through them. (Pascal VOC format)\n",
    "    try:\n",
    "        #Try iterating through the tag (For images with more than 1 obj).\n",
    "        for each_obj in doc['annotation']['object']:\n",
    "            \n",
    "            obj_class = each_obj['name'].lower() #get the label for the object and lowercase the string.\n",
    "            \n",
    "            if obj_class in excluded_classes:\n",
    "                continue\n",
    "\n",
    "            #Pascal VOC's format to denote bounding boxes are to denote the top left part of the box and the bottom right of the box.\n",
    "            #the coordinates are in terms of x and y axis for both part of the box.\n",
    "            x_min = float(each_obj['bndbox']['xmin']) #top left x-axis coordinate.\n",
    "            x_max = float(each_obj['bndbox']['xmax']) #bottom right x-axis coordinate.\n",
    "            y_min = float(each_obj['bndbox']['ymin']) #top left y-axis coordinate.\n",
    "            y_max = float(each_obj['bndbox']['ymax']) #bottom right y-axis coordinate.\n",
    "\n",
    "        ##################################################################################\n",
    "        #We want to make sure the coordinates are resized according to the resized image.#\n",
    "        ##################################################################################\n",
    "\n",
    "            #All the images will be resized to a fixed size in order to be fixed-size inputs to the neural network model.\n",
    "            #Therefore, we need to resize the coordinates as well since the coordinates above is based on the original size of the images.\n",
    "\n",
    "            #In order to find the resized coordinates, we must multiply the ratio of the resized image compared to its original to the coordinates.\n",
    "            x_min = float((image_width/ori_img_width)*x_min)\n",
    "            y_min = float((image_height/ori_img_height)*y_min)\n",
    "            x_max = float((image_width/ori_img_width)*x_max)\n",
    "            y_max = float((image_height/ori_img_height)*y_max)\n",
    "\n",
    "            generated_box_info = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "\n",
    "            index = classes.index(obj_class) #get the index of the object's class.\n",
    "\n",
    "            #append each object's class label and the bounding box label (converted to Faster R-CNN format) into the list initialized earlier.\n",
    "            class_label.append(index)\n",
    "            bbox_label.append(np.asarray(generated_box_info, dtype='float32'))\n",
    "\n",
    "    except TypeError as e : #happens when the iteration through the tag fails due to only 1 object being in the image.\n",
    "\n",
    "        #SAME PROCEDURE AS ABOVE !  \n",
    "\n",
    "        #Getting these information from the XML file differs compared to above,\n",
    "        obj_class = doc['annotation']['object']['name']\n",
    "        \n",
    "        if not obj_class in excluded_classes:\n",
    "                        \n",
    "            x_min = float(doc['annotation']['object']['bndbox']['xmin']) \n",
    "            x_max = float(doc['annotation']['object']['bndbox']['xmax']) \n",
    "            y_min = float(doc['annotation']['object']['bndbox']['ymin']) \n",
    "            y_max = float(doc['annotation']['object']['bndbox']['ymax']) \n",
    "\n",
    "            x_min = float((image_width/ori_img_width)*x_min)\n",
    "            y_min = float((image_height/ori_img_height)*y_min)\n",
    "            x_max = float((image_width/ori_img_width)*x_max)\n",
    "            y_max = float((image_height/ori_img_height)*y_max)\n",
    "\n",
    "            generated_box_info = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "            #Get the index of the class\n",
    "            index = classes.index(obj_class) \n",
    "\n",
    "            class_label.append(index)\n",
    "            bbox_label.append(np.asarray(generated_box_info, dtype='float32'))\n",
    "\n",
    "\n",
    "    return class_label, np.asarray(bbox_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class K_Means:\n",
    "#     '''\n",
    "#     K-Means Algorithm.\n",
    "#     '''\n",
    "    \n",
    "#     def __init__(self, k, data_points):\n",
    "#         '''\n",
    "#         Initialize parameters, centroids and restructure data points.\n",
    "#         '''\n",
    "        \n",
    "#         self.k = k\n",
    "#         self.data_points = data_points\n",
    "#         self.flag = False #used for the stopping criteria of the algorithm later\n",
    "        \n",
    "#         self.centroids = []\n",
    "#         for i in range(self.k):\n",
    "#             self.centroids.append(self.data_points[i, :]) #the centroid is chosen from the first k data points.\n",
    "        \n",
    "#         #in order to identify a data point with a certain centroid, we'll use an extra column to represent\n",
    "#         #the index of the centroid from the list of centroids. We'll create a new column in every data point\n",
    "#         #and initialize them to 0 at first.\n",
    "        \n",
    "#         #same shape as data_points but with an extra column in the second dimension.\n",
    "#         init_zeros = np.zeros((self.data_points.shape[0], self.data_points.shape[1]+1)) \n",
    "#         init_zeros[:,:-1] = self.data_points #fill everything except the last column with the data points\n",
    "#         self.data_points = init_zeros\n",
    "    \n",
    "#     def calculate_iou(self, box1, box2, area1=None, area2=None):\n",
    "#         '''\n",
    "#         Inputs two boxes (each with top-left-x, top-left-y, btm-right-x, btm-right-y coordinates).\n",
    "#         If the area of one or two of the boxes are known, it can be passed here as well.\n",
    "#         Calculates the IoU between the two boxes and returns the IoU.\n",
    "#         '''\n",
    "\n",
    "#         #Get the area of the boxes\n",
    "#         if area1 is None:\n",
    "#             #Area of the box1. +1 since index starts from 0.\n",
    "#             area1 = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
    "\n",
    "#         if area2 is None:\n",
    "#             #Area of the box 2. +1 since index starts from 0.\n",
    "#             area2 = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
    "\n",
    "#         #Determine the intersection rectangle.\n",
    "#         int_rect_top_left_x = max(box1[0], box2[0])\n",
    "#         int_rect_top_left_y = max(box1[1], box2[1])\n",
    "#         int_rect_btm_rght_x = min(box1[2], box2[2])\n",
    "#         int_rect_btm_rght_y = min(box1[3], box2[3])\n",
    "\n",
    "#         #if the boxes do not intersect, the difference will be < 0. Hence we pick 0 in those cases.\n",
    "#         int_rect_area = max(0, int_rect_btm_rght_x - int_rect_top_left_x + 1)*max(0, int_rect_btm_rght_y - int_rect_top_left_y)\n",
    "\n",
    "#         #Calculate the IoU.\n",
    "#         try:\n",
    "#             intersect_over_union = float(int_rect_area / (area1 + area2 - int_rect_area))\n",
    "#         except ZeroDivisionError:\n",
    "\n",
    "#             intersect_over_union = 0\n",
    "        \n",
    "    \n",
    "#         return intersect_over_union\n",
    "    \n",
    "    \n",
    "#     def __call__(self):\n",
    "#         '''\n",
    "#         Step 1 : Calculates the distance between each data point and each centroid and assign the closest\n",
    "#                  centroid to the data point.\n",
    "#         Step 2 : Calculates the mean of every data point in each cluster and assign the value of the mean to\n",
    "#                  the associated centroid.\n",
    "#         Step 3 : Repeat Step 1 and 2 until there are no more changes to be done to the centroids.\n",
    "#         '''\n",
    "#         while (not self.flag): #iterates until the flag is set to True.\n",
    "            \n",
    "#             for data in self.data_points:\n",
    "                \n",
    "#                 #initialize the variable and the index of the centroid in the data point array.\n",
    "#                 init_iou = 1 - self.calculate_iou(data[:-1], self.centroids[0]) \n",
    "#                 data[-1] = 0\n",
    "                \n",
    "#                 for index,centroid in enumerate(self.centroids[1:], start=1):\n",
    "                    \n",
    "#                     iou = 1 - self.calculate_iou(data[:-1], centroid)\n",
    "                    \n",
    "#                     if iou < init_iou:\n",
    "#                         init_iou = iou\n",
    "#                         data[-1] = index\n",
    "            \n",
    "            \n",
    "#             self.flag = True #this will be changed if the centroids are updated.\n",
    "#             for cluster_index in range(self.k):\n",
    "                \n",
    "#                 #get all the data points that belong to a certain cluster.\n",
    "#                 cluster_data = self.data_points[self.data_points[:, -1] == cluster_index]\n",
    "                \n",
    "#                 mean = np.median(cluster_data[:,:-1], axis=0) #calculates the mean of the data points on axis 0.\n",
    "                \n",
    "#                 #if the current cluster value and the mean value is the same, skip the loop.\n",
    "#                 #if this happens for all the centroids, the flag value will remain true and therefore\n",
    "#                 #meeting the stopping criteria.\n",
    "#                 print(mean)\n",
    "#                 if np.array_equal(self.centroids[cluster_index], mean):\n",
    "#                     continue\n",
    "                \n",
    "#                 #updates the centroid with the new mean value and change the flag value to continue the loop.\n",
    "#                 self.centroids[cluster_index] = mean\n",
    "#                 self.flag = False\n",
    "        \n",
    "#         return (self.data_points, self.centroids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_Means:\n",
    "    \n",
    "    \n",
    "    def __init__(self, k, boxes):\n",
    "        \n",
    "        self.k = k\n",
    "        self.boxes = boxes\n",
    "        self.rows = self.boxes.shape[0]\n",
    "        self.distances = np.empty((self.rows, self.k))\n",
    "        self.last_centroids = np.zeros((self.rows,))\n",
    "        \n",
    "        self.boxes = self.process_boxes(self.boxes)\n",
    "        self.centroids = []\n",
    "        for i in range(self.k):\n",
    "            self.centroids.append(self.boxes[i,:])\n",
    "        \n",
    "        self.centroids = np.asarray(self.centroids, dtype=np.float32)\n",
    "        \n",
    "    def process_boxes(self, boxes):\n",
    "        \n",
    "        new_boxes = boxes.copy()\n",
    "        for row in range(self.rows):\n",
    "#             width = np.abs(new_boxes[row][2] - new_boxes[row][0])\n",
    "#             height = np.abs(new_boxes[row][3] - new_boxes[row][1])\n",
    "            \n",
    "#             new_boxes[row][0] = new_boxes[row][0] / width\n",
    "#             new_boxes[row][1] = new_boxes[row][1] / height\n",
    "#             new_boxes[row][2] = new_boxes[row][2] / width\n",
    "#             new_boxes[row][3] = new_boxes[row][3] / height\n",
    "            \n",
    "            new_boxes[row][2] = np.abs(new_boxes[row][2] - new_boxes[row][0])\n",
    "            new_boxes[row][3] = np.abs(new_boxes[row][3] - new_boxes[row][1])\n",
    "        \n",
    "        return np.delete(new_boxes, [0,1], axis=1)\n",
    "    \n",
    "    def iou(self, box, centroids):\n",
    "        \n",
    "        x = np.minimum(centroids[:, 0], box[0])\n",
    "        y = np.minimum(centroids[:, 1], box[1])\n",
    "        \n",
    "        if np.count_nonzero(x == 0) > 0 or np.count_nonzero(y == 0) > 0:\n",
    "            raise ValueError(\"The given box has no area!\")\n",
    "        \n",
    "        intersection_area = x * y\n",
    "        box_area = box[0] * box[1]\n",
    "        centroid_area = centroids[:, 0] * centroids[:, 1]\n",
    "        \n",
    "        IoUs = intersection_area / (box_area + centroid_area - intersection_area)\n",
    "        \n",
    "        return IoUs\n",
    "    \n",
    "    def __call__(self):\n",
    "        \n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            for row in range(self.rows):\n",
    "                self.distances[row] = 1 - self.iou(self.boxes[row], self.centroids)\n",
    "            \n",
    "            nearest_centroids = np.argmin(self.distances, axis=1)\n",
    "            \n",
    "            if (self.last_centroids == nearest_centroids).all():\n",
    "                break\n",
    "            \n",
    "            for cluster in range(self.k):\n",
    "                self.centroids[cluster] = np.mean(self.boxes[nearest_centroids == cluster], axis=0)\n",
    "                \n",
    "            self.last_centroids = nearest_centroids\n",
    "        \n",
    "        return self.centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40138"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_boxes_array = []\n",
    "for i in range(total_images):\n",
    "    \n",
    "    true_labels, gt_boxes = get_labels_from_xml(xml_file_path=list_annotations[i])\n",
    "    if len(gt_boxes) == 1:\n",
    "        \n",
    "        gt_boxes_array.append(np.asarray(gt_boxes[0]))\n",
    "    else:\n",
    "        for i in range(len(gt_boxes)):\n",
    "            gt_boxes_array.append(np.asarray(gt_boxes[i]))\n",
    "\n",
    "len(gt_boxes_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_boxes_array = np.asarray(gt_boxes_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = K_Means(k=5, boxes=gt_boxes_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = kmeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[108.39007 , 220.13586 ],\n",
       "       [249.4025  , 266.85065 ],\n",
       "       [195.40692 , 130.10524 ],\n",
       "       [ 62.328476, 112.9795  ],\n",
       "       [ 24.958393,  41.56217 ]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = np.zeros((320,320,3))\n",
    "cv2.rectangle(img, (4,4), (int(n[0][0])+4,int(n[0][1])+4), (255,255,255), 2)\n",
    "cv2.rectangle(img, (4,4), (int(n[1][0])+4,int(n[1][1])+4),  (255,255,0), 2)\n",
    "cv2.rectangle(img, (4,4), (int(n[2][0])+4,int(n[2][1])+4),  (0,0,255), 2)\n",
    "cv2.rectangle(img, (4,4), (int(n[3][0])+4,int(n[3][1])+4),  (255,0,0), 2)\n",
    "cv2.rectangle(img, (4,4), (int(n[4][0])+4,int(n[4][1])+4),  (100,0,50), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(cv2.flip(img.astype(np.uint8),0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15 53]\n",
      " [ 8 16]\n",
      " [43 47]\n",
      " [ 0  6]\n",
      " [91 60]\n",
      " [95 38]\n",
      " [57 37]\n",
      " [33 76]\n",
      " [39 49]\n",
      " [10  3]] [24.5]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randint(100,size=(10,2))\n",
    "b = np.asarray([24.5])\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.5, 16. , 24.5, 24.5, 24.5,  9. , 24.5, 24.5, 24.5, 15. ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.minimum(a[:,1], b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
